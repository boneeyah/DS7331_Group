{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aca46ae-a0f7-4b71-8268-b0dd98d2be96",
   "metadata": {},
   "source": [
    "# MiniLab1 - \n",
    "## AIRBnB Listings for Los Angeles Area\n",
    "### Jason McDonald, Miguel Bonilla, Zachary Bunn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366929eb-21b5-4577-ab05-4bd14d1b095c",
   "metadata": {},
   "source": [
    "This notebook explores the application of logistic regression, Support Vector Machine (SVM), and Gradient Optimization as classifiers to Superhost status of AirBnB listings (a binary categorical response)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c417ca4e-c052-44e2-9f4b-3e0c84e88cdc",
   "metadata": {},
   "source": [
    "### Data Wrangling\n",
    "\n",
    "First, we begin by cleaning up the data and imputing missing values for certain variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9722752c-e0e1-4862-bece-058690ae9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "df = pd.read_csv(\"https://media.githubusercontent.com/media/boneeyah/DS7331_Group/main/Data_Files/airbnb_los_angeles.csv\")\n",
    "#df = pd.read_csv(\"Data_Files/airbnb_los_angeles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e8676f5-9c08-472a-94b0-7cf8ac42ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop variables which won't be useful for the classification model\n",
    "for col in [\n",
    "    'listing_url','scrape_id','last_scraped','description','neighborhood_overview','picture_url','host_url','host_about','host_response_time','host_response_rate','host_acceptance_rate',\n",
    "    'host_thumbnail_url','host_picture_url','host_verifications','host_has_profile_pic','bathroom_text','host_listings_count','host_neighbourhood','bathrooms','minimum_minimum_nights',\n",
    "    'maximum_minimum_nights','minimum_maximum_nights','maximum_maximum_nights','minimum_nights_avg_ntm','maximum_nights_avg_ntm','calendar_updated','availability_30','availability_60',\n",
    "    'availability_90','availability_365','calendar_last_scraped','number_of_reviews_ltm','number_of_reviews_l30d','review_scores_accuracy','review_scores_communication','review_scores_cleanliness',\n",
    "    'review_scores_checkin','review_scores_value','review_scores_location','calculated_host_listings_count_entire_homes','calculated_host_listings_count_private_rooms',\n",
    "    'calculated_host_listings_count_shared_rooms','reviews_per_month','neighbourhood','neighbourhood_group_cleansed', 'first_review','last_review','minimum_nights','maximum_nights','license','host_total_listings_count'\n",
    "]:\n",
    "    if col in df:\n",
    "        del df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84031bd0-0813-4264-81fa-f3548cb72b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nulls\n",
    "df = df[~df.review_scores_rating.isnull() & ~df.bathrooms_text.isnull() & ~df.host_since.isnull() & ~df.host_location.isnull()]\n",
    "\n",
    "## getting property type from string\n",
    "types = ['Private room', 'Entire', 'Room in hotel','Room','Shared room']\n",
    "pat = '|'.join(r\"\\b{}\\b\".format(x) for x in types)\n",
    "\n",
    "df['property_type']= df['property_type'].str.extract('('+ pat + ')', expand = False)\n",
    "df['property_type'] = (df.property_type.\n",
    "                       fillna(value = 'other').\n",
    "                       replace(['Entire','Room in hotel'],['Entire unit','Hotel room']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e58420e-ee3a-4444-a029-c8234aad0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute values based on median\n",
    "df['beds'] = df[['accommodates','beds']].groupby(by = 'accommodates').transform(lambda grp: grp.fillna(grp.median()))\n",
    "df_grouped = df.groupby(by = ['property_type','beds'])\n",
    "df_imputed = df_grouped[['beds','bedrooms']].transform(lambda grp: grp.fillna(grp.median()))\n",
    "\n",
    "index = df_imputed[df_imputed.bedrooms.isnull()].index\n",
    "df = df.drop(index= index)\n",
    "\n",
    "df['imputed']=df_imputed[['bedrooms']]\n",
    "\n",
    "# replace 'bedrooms' column with imputed column and deleting the duplicated column\n",
    "df['bedrooms'] = df['imputed']\n",
    "del df['imputed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e63e7586-90a1-4694-aa9f-54a8bdf0c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now fixing dtypes for attributes\n",
    "df['host_since'] = pd.to_datetime(df.host_since)\n",
    "df['price'] = df['price'].replace('[\\$,]','',regex = True).astype(float)\n",
    "df['bathrooms_text'] = df['bathrooms_text'].replace(['Half-bath', 'Shared half-bath', 'Private half-bath'],['0.5 bath','0.5 shared bath', '0.5 private bath'])\n",
    "df_bathrooms = df['bathrooms_text'].str.split(n=1, expand=True).rename(columns = {0:'bathroom_number',1:'bathroom_type'})\n",
    "df_bathrooms['bathroom_type'] = df_bathrooms.bathroom_type.fillna(value = 'bath')\n",
    "df_bathrooms['bathroom_type'] = df_bathrooms['bathroom_type'].replace(['baths','shared baths'],['bath','shared bath'])\n",
    "df_bathrooms['bathroom_number'] = df_bathrooms['bathroom_number'].astype('float')\n",
    "df.insert(15, 'bathroom_number',df_bathrooms['bathroom_number'])\n",
    "df.insert(16, 'bathroom_type', df_bathrooms['bathroom_type'])\n",
    "del df['bathrooms_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6705188f-0607-439b-9898-e239313cb693",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter out price outliers\n",
    "df = df[(df.beds<10) & (df.price<750)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef94df2-03a6-4c1f-a16b-8f37ecc0d208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30580 entries, 0 to 30579\n",
      "Data columns (total 25 columns):\n",
      " #   Column                          Non-Null Count  Dtype         \n",
      "---  ------                          --------------  -----         \n",
      " 0   id                              30580 non-null  int64         \n",
      " 1   name                            30580 non-null  object        \n",
      " 2   host_id                         30580 non-null  int64         \n",
      " 3   host_name                       30580 non-null  object        \n",
      " 4   host_since                      30580 non-null  datetime64[ns]\n",
      " 5   host_location                   30580 non-null  object        \n",
      " 6   host_is_superhost               30580 non-null  object        \n",
      " 7   host_identity_verified          30580 non-null  object        \n",
      " 8   neighbourhood_cleansed          30580 non-null  object        \n",
      " 9   latitude                        30580 non-null  float64       \n",
      " 10  longitude                       30580 non-null  float64       \n",
      " 11  property_type                   30580 non-null  object        \n",
      " 12  room_type                       30580 non-null  object        \n",
      " 13  accommodates                    30580 non-null  int64         \n",
      " 14  bathroom_number                 30580 non-null  float64       \n",
      " 15  bathroom_type                   30580 non-null  object        \n",
      " 16  bedrooms                        30580 non-null  float64       \n",
      " 17  beds                            30580 non-null  float64       \n",
      " 18  amenities                       30580 non-null  object        \n",
      " 19  price                           30580 non-null  float64       \n",
      " 20  has_availability                30580 non-null  object        \n",
      " 21  number_of_reviews               30580 non-null  int64         \n",
      " 22  review_scores_rating            30580 non-null  float64       \n",
      " 23  instant_bookable                30580 non-null  object        \n",
      " 24  calculated_host_listings_count  30580 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(7), int64(5), object(12)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "436fc516-506b-407a-92c2-42b1be9fb92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <td>30580</td>\n",
       "      <td>29942</td>\n",
       "      <td>Boutique Hostel</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_name</th>\n",
       "      <td>30580</td>\n",
       "      <td>6604</td>\n",
       "      <td>David</td>\n",
       "      <td>249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_location</th>\n",
       "      <td>30580</td>\n",
       "      <td>980</td>\n",
       "      <td>Los Angeles, California, United States</td>\n",
       "      <td>10613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_is_superhost</th>\n",
       "      <td>30580</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>20236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>host_identity_verified</th>\n",
       "      <td>30580</td>\n",
       "      <td>2</td>\n",
       "      <td>t</td>\n",
       "      <td>26080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neighbourhood_cleansed</th>\n",
       "      <td>30580</td>\n",
       "      <td>302</td>\n",
       "      <td>Long Beach</td>\n",
       "      <td>1163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>property_type</th>\n",
       "      <td>30580</td>\n",
       "      <td>6</td>\n",
       "      <td>Entire unit</td>\n",
       "      <td>21090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>room_type</th>\n",
       "      <td>30580</td>\n",
       "      <td>4</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>21433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bathroom_type</th>\n",
       "      <td>30580</td>\n",
       "      <td>3</td>\n",
       "      <td>bath</td>\n",
       "      <td>22738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amenities</th>\n",
       "      <td>30580</td>\n",
       "      <td>28937</td>\n",
       "      <td>[\"Hangers\", \"First aid kit\", \"Heating\", \"Fire ...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has_availability</th>\n",
       "      <td>30580</td>\n",
       "      <td>2</td>\n",
       "      <td>t</td>\n",
       "      <td>28785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instant_bookable</th>\n",
       "      <td>30580</td>\n",
       "      <td>2</td>\n",
       "      <td>f</td>\n",
       "      <td>19767</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        count unique  \\\n",
       "name                    30580  29942   \n",
       "host_name               30580   6604   \n",
       "host_location           30580    980   \n",
       "host_is_superhost       30580      2   \n",
       "host_identity_verified  30580      2   \n",
       "neighbourhood_cleansed  30580    302   \n",
       "property_type           30580      6   \n",
       "room_type               30580      4   \n",
       "bathroom_type           30580      3   \n",
       "amenities               30580  28937   \n",
       "has_availability        30580      2   \n",
       "instant_bookable        30580      2   \n",
       "\n",
       "                                                                      top  \\\n",
       "name                                                      Boutique Hostel   \n",
       "host_name                                                           David   \n",
       "host_location                      Los Angeles, California, United States   \n",
       "host_is_superhost                                                       f   \n",
       "host_identity_verified                                                  t   \n",
       "neighbourhood_cleansed                                         Long Beach   \n",
       "property_type                                                 Entire unit   \n",
       "room_type                                                 Entire home/apt   \n",
       "bathroom_type                                                        bath   \n",
       "amenities               [\"Hangers\", \"First aid kit\", \"Heating\", \"Fire ...   \n",
       "has_availability                                                        t   \n",
       "instant_bookable                                                        f   \n",
       "\n",
       "                         freq  \n",
       "name                       46  \n",
       "host_name                 249  \n",
       "host_location           10613  \n",
       "host_is_superhost       20236  \n",
       "host_identity_verified  26080  \n",
       "neighbourhood_cleansed   1163  \n",
       "property_type           21090  \n",
       "room_type               21433  \n",
       "bathroom_type           22738  \n",
       "amenities                  40  \n",
       "has_availability        28785  \n",
       "instant_bookable        19767  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# breakdown of categorical variables with number of levels (unique)\n",
    "df.iloc[:,[1,3,5,6,7,8,11,12,15,18,20,23]].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51119bb3-edc7-45e7-996c-7db0ce57471e",
   "metadata": {},
   "source": [
    "This table shows the number of unique levels for categorical variables in our dataset, as well as the most common level of each variable along with its frequency count.\n",
    "From this table, we can see that about 1/3 of listings (10613/30580) correspond to a host who has Superhost status. Additionally the table shows that amenities has 28937 unique levels, given the way these are listed for each listing, a decission was made to drop the variable for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de56075-d76d-4607-a50a-9cedc07fe04b",
   "metadata": {},
   "source": [
    "# Create Models\n",
    "\n",
    "\n",
    "### Rubric Note: Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use a 80/20 train/test split for your data). Adjust parameters of the model to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. THat is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. FOr many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab7224-94ee-4339-bdb8-22908661819c",
   "metadata": {},
   "source": [
    "To be able to use categorical variables when fitting our models, we will first encode these variables using LabelEncoder and OneHotEncoder, which convert variables into categorical variables represented by binary values of 0 and 1. LabelEncoder can be used to convert binary categories into binary values of 0 and 1 for each level of the original variable. OneHotEncoder can be used to covert variables with more than two categorical variables, by creating n-1 columns (where n corresponds to the number of levels in a category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "843368e8-f02d-40f0-abca-06a0c57cd36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    20236\n",
       "1    10344\n",
       "Name: host_is_superhost, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model will focus on classifying superhost status\n",
    "# since we're encoding with binary response, we can use labelencoder from sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['host_is_superhost'] = label_encoder.fit_transform(df['host_is_superhost'])\n",
    "df.host_is_superhost.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f9113-5004-48f9-b9ad-ae125a31f19d",
   "metadata": {},
   "source": [
    "The value of 0 corresponds to the status of 'f' of the original variable, a quick check shows that the value matches that which was previously found (20236 for 'f')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e211631-a474-45c2-bb8c-34bb9a4ea7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "host_identity_verified\n",
      "1    26080\n",
      "0     4500\n",
      "Name: host_identity_verified, dtype: int64\n",
      "has_availability\n",
      "1    28785\n",
      "0     1795\n",
      "Name: has_availability, dtype: int64\n",
      "instant_bookable\n",
      "0    19767\n",
      "1    10813\n",
      "Name: instant_bookable, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## encoding binary categorical variables\n",
    "# encode identity verified\n",
    "df['host_identity_verified'] = label_encoder.fit_transform(df['host_identity_verified'])\n",
    "print('host_identity_verified')\n",
    "print(df.host_identity_verified.value_counts())\n",
    "\n",
    "#encode has_availability\n",
    "df['has_availability'] = label_encoder.fit_transform(df['has_availability'])\n",
    "print('has_availability')\n",
    "print(df.has_availability.value_counts())\n",
    "\n",
    "#encode instant bookable\n",
    "df['instant_bookable'] = label_encoder.fit_transform(df['instant_bookable'])\n",
    "print('instant_bookable')\n",
    "print(df.instant_bookable.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6073dca8-6d90-4e93-9362-7aeed4d63998",
   "metadata": {},
   "source": [
    "Encoding the rest of the binary categorical variables similarly shows the new binary levels match the categorical levels of the original variables.\n",
    "\n",
    "In order to gain more useful insight from host location, we made the decission to create a new feature to replace it based on whether the host is local or not. We utilized a new table containing 100+ neighborhoods in the Los Angeles area, and matched those to host location to determine if a host is local to the LA area. We then used LabelEncoder to covert it into binary values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38ef38fe-7d74-4925-b8c8-63597a58be4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    19056\n",
       "1    11524\n",
       "Name: host_is_local, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## changing host_location to a binary feature of is_local using a list of LA area neighborhoods that will be used to extract from location str\n",
    "#los_angeles = pd.read_csv('Data_Files/LosAngelesNeighborhoods.csv')\n",
    "los_angeles = pd.read_csv('https://raw.githubusercontent.com/boneeyah/DS7331_Group/main/Data_Files/LosAngelesNeighborhoods.csv')\n",
    "los_angeles = los_angeles.iloc[:,0].tolist()\n",
    "\n",
    "pattern = '|'.join(los_angeles)\n",
    "df['host_is_local'] = df['host_location'].str.contains(pattern)\n",
    "\n",
    "df['host_is_local'] = label_encoder.fit_transform(df['host_is_local'])\n",
    "df.host_is_local.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5423f85-a04d-411a-b330-614478dfffc2",
   "metadata": {},
   "source": [
    "With the newly created variable host_is_local replacing host_location, we now have a variable with 2 levels, as opposed to the original with over 900 categories.\n",
    "\n",
    "To convert host_since from datetime to a useable format, we made the decission to create a new variable, host_for, corresponding to the number of months since the host first listed properties on AirBnB. This was accomplished by subtracting the host_since date from the date on which the data was pulled from AirBnB (06-06-2022)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7be9bb4-5b2e-4ef2-a251-c4d88f1dd246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30580 entries, 0 to 30579\n",
      "Data columns (total 25 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   id                              30580 non-null  int64  \n",
      " 1   name                            30580 non-null  object \n",
      " 2   host_id                         30580 non-null  int64  \n",
      " 3   host_name                       30580 non-null  object \n",
      " 4   host_is_superhost               30580 non-null  int64  \n",
      " 5   host_identity_verified          30580 non-null  int64  \n",
      " 6   neighbourhood_cleansed          30580 non-null  object \n",
      " 7   latitude                        30580 non-null  float64\n",
      " 8   longitude                       30580 non-null  float64\n",
      " 9   property_type                   30580 non-null  object \n",
      " 10  room_type                       30580 non-null  object \n",
      " 11  accommodates                    30580 non-null  int64  \n",
      " 12  bathroom_number                 30580 non-null  float64\n",
      " 13  bathroom_type                   30580 non-null  object \n",
      " 14  bedrooms                        30580 non-null  float64\n",
      " 15  beds                            30580 non-null  float64\n",
      " 16  amenities                       30580 non-null  object \n",
      " 17  price                           30580 non-null  float64\n",
      " 18  has_availability                30580 non-null  int64  \n",
      " 19  number_of_reviews               30580 non-null  int64  \n",
      " 20  review_scores_rating            30580 non-null  float64\n",
      " 21  instant_bookable                30580 non-null  int64  \n",
      " 22  calculated_host_listings_count  30580 non-null  int64  \n",
      " 23  host_is_local                   30580 non-null  int64  \n",
      " 24  host_for                        30580 non-null  float64\n",
      "dtypes: float64(8), int64(10), object(7)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "## changing host_since to host_for to get a a numerical variable that corresponds to length in months\n",
    "## data is from June 6 2022, 06-06-2022\n",
    "end_date = pd.to_datetime('06-06-2022', format= \"%m-%d-%Y\")\n",
    "df['host_for'] = (end_date-df.host_since)/np.timedelta64(1,'M')\n",
    "\n",
    "df = df.drop(columns= ['host_location', 'host_since']) #drop old host_location and host_since columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feba56b4-49c3-495b-be91-97a3c9770f48",
   "metadata": {},
   "source": [
    "This table shows the remaining variables which need to be One-Hot Encoded (neighbourhood_cleansed, property_type, room_type, bathroom_type) or dropped since they won't be useful for our modeling (name, host_name, amenities, id, host_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70f1c92c-c941-4257-aab5-cf4528792798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding other categorical variables as a non-sparse dataframe\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "df_temp = df[['id','neighbourhood_cleansed','property_type', 'room_type', 'bathroom_type']] # to get only the cat variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c799bcac-826c-4c0b-be36-f697cf28c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(drop='first',sparse=False)\n",
    "feature_arr = ohe.fit_transform(df_temp[['neighbourhood_cleansed','property_type', 'room_type', 'bathroom_type']])\n",
    "feature_labels = ohe.get_feature_names_out()\n",
    "\n",
    "#create dataframe with features\n",
    "df_temp = pd.DataFrame(feature_arr, columns= feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2eec75e9-070e-450a-9a28-11a916f1ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop original columns and join with new onehotencoded columns\n",
    "df = df.drop(columns = ['neighbourhood_cleansed','property_type', 'room_type', 'bathroom_type','amenities','name','host_name','id','host_id','latitude','longitude']).join(df_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cefb4eeb-fea3-43f3-b4f2-238a5baa6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### different method I found first, commented out for now, this could be used to transform within the original dataset saving one step\n",
    "### but it's easy enough to just fit a new dataframe and then add to original and drop original columns instead\n",
    "#transformer = make_column_transformer((OneHotEncoder(drop = 'first',sparse=False), ['neighbourhood_cleansed','property_type', 'room_type', 'bathroom_type']), remainder='passthrough')\n",
    "#df_temp = transformer.fit_transform(df_temp)\n",
    "#df_temp = pd.DataFrame(df_temp, columns=transformer.get_feature_names_out())\n",
    "#print(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7387da7-ff79-4f7c-98d9-56b7b97a8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean up temporary files\n",
    "del df_temp, feature_arr, feature_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83169115-f751-4728-b3ae-15cda28cbd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30580 entries, 0 to 30579\n",
      "Columns: 325 entries, host_is_superhost to bathroom_type_shared bath\n",
      "dtypes: float64(317), int64(8)\n",
      "memory usage: 75.8 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b76d920-2e73-4dca-a840-1abb138114d6",
   "metadata": {},
   "source": [
    "After performing One-hot Encoding, our resulting dataframe has 325 variables with 30580 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94150f29-3865-4d72-a8dd-5b4d1fc290d8",
   "metadata": {},
   "source": [
    "### Logistic Regression Model\n",
    "We will train our models using an 80-20 train-test split, using 10-fold cross validation from scikit-learn. Additionally, we will rescale our predictors using StandardScaler to ensure all variables are on the same scale. We will repeat the process 3 times utilizing predetermined seeds to verify the model performs consistently and our results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "374dac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages we'll be using in the model building sections.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#dividing dataset into features and labels and using a standard scaler to scale the x values\n",
    "X = df.loc[:,df.columns != 'host_is_superhost']\n",
    "X_sc = StandardScaler().fit_transform(X) #scaled X values\n",
    "X_sc = pd.DataFrame(X_sc,columns=X.columns)\n",
    "y = df.loc[:, df.columns == 'host_is_superhost']\n",
    "\n",
    "\n",
    "seed = [10,2,2022] #seed to be used, it will be assigned as random_state when splitting instead of using np to set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1dfdda2-c8c6-4030-be9f-2808353fc4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Iteration   0 ---------\n",
      "\n",
      "Confusion Matrix\n",
      "       True  False\n",
      "True    892   1130\n",
      "False   589   3505\n",
      "________________________________\n",
      "accuracy: 0.7189339437540876\n",
      "sensitivity: 0.6022957461174882\n",
      "specificity: 0.756202804746494 \n",
      "\n",
      "---------Iteration   1 ---------\n",
      "\n",
      "Confusion Matrix\n",
      "       True  False\n",
      "True    937   1112\n",
      "False   559   3508\n",
      "________________________________\n",
      "accuracy: 0.7267822105951602\n",
      "sensitivity: 0.6263368983957219\n",
      "specificity: 0.7593073593073593 \n",
      "\n",
      "---------Iteration   2 ---------\n",
      "\n",
      "Confusion Matrix\n",
      "       True  False\n",
      "True    910   1117\n",
      "False   527   3562\n",
      "________________________________\n",
      "accuracy: 0.7311968606932636\n",
      "sensitivity: 0.6332637439109255\n",
      "specificity: 0.761273776447959 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### performing 80/20 split using sklearn\n",
    "\n",
    "iter_num =0\n",
    "for i in seed:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_sc,y, random_state=i, train_size=.8, shuffle=True) #X_train is scaled since X_sc is used for X\n",
    "    lor_clf = LogisticRegressionCV(cv=10, max_iter = 300, random_state = i)\n",
    "    lor_clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    y_hat = lor_clf.predict(X_test)\n",
    "\n",
    "    cm = confusion_matrix(y_test,y_hat, labels=[1,0])\n",
    "    print('---------Iteration  ',iter_num,'---------')\n",
    "    print('')\n",
    "    print('Confusion Matrix')\n",
    "    print(pd.DataFrame(cm,index= ['True','False'], columns=['True','False']))\n",
    "    print('________________________________')\n",
    "    print('accuracy:', (cm[0,0]+cm[1,1])/sum(cm.ravel()))\n",
    "    print('sensitivity:', cm[0,0]/(cm[0,0]+cm[1,0]))\n",
    "    print('specificity:', cm[1,1]/(cm[1,1]+cm[0,1]),'\\n')\n",
    "    iter_num=iter_num+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b2b5c9-5a20-4e49-93fc-8246adc8a5d4",
   "metadata": {},
   "source": [
    "This model yields results with good accuracy and specificity (correctly identifying Superhost status false when the host is not a Superhost). However, the model's sensitivity is lower than the others at 60-63%. This is potentially due to the Superhost status not being balanced at a close to 50-50 split. When utilizing LabelEncoder on the variable we identified that close to 34% of listings corresponded to a host with Superhost status. We will therefore adjust the cutoff value to a higher threshold to account for this imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1c0017f-b9b2-43fd-bba7-b5420a7644ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Iteration   0 ---------\n",
      "\n",
      "Confusion Matrix\n",
      "       True  False\n",
      "True    449   1573\n",
      "False   187   3907\n",
      "________________________________\n",
      "accuracy: 0.7122302158273381\n",
      "sensitivity: 0.7059748427672956\n",
      "specificity: 0.7129562043795621 \n",
      "\n",
      "---------Iteration   1 ---------\n",
      "\n",
      "Confusion Matrix\n",
      "       True  False\n",
      "True    421   1628\n",
      "False   158   3909\n",
      "________________________________\n",
      "accuracy: 0.7079790712884239\n",
      "sensitivity: 0.7271157167530224\n",
      "specificity: 0.7059779664078021 \n",
      "\n",
      "---------Iteration   2 ---------\n",
      "\n",
      "Confusion Matrix\n",
      "       True  False\n",
      "True    429   1598\n",
      "False   149   3940\n",
      "________________________________\n",
      "accuracy: 0.7143557880967953\n",
      "sensitivity: 0.7422145328719724\n",
      "specificity: 0.7114481762369086 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### performing 80/20 split using sklearn\n",
    "cutoff = .63\n",
    "\n",
    "\n",
    "iter_num =0\n",
    "for i in seed:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_sc,y, random_state=i, train_size=.8, shuffle=True) #X_train is scaled since X_sc is used for X\n",
    "    lor_clf = LogisticRegressionCV(cv=10, max_iter = 300, random_state = i)\n",
    "    lor_clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    y_hat = lor_clf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "    response = pd.DataFrame(y_hat, columns=['F','T'])\n",
    "    y_preds=(response['T']>cutoff).astype('int')\n",
    "\n",
    "    cm = confusion_matrix(y_test,y_preds, labels=[1,0])\n",
    "    print('---------Iteration  ',iter_num,'---------')\n",
    "    print('')\n",
    "    print('Confusion Matrix')\n",
    "    print(pd.DataFrame(cm,index= ['True','False'], columns=['True','False']))\n",
    "    print('________________________________')\n",
    "    print('accuracy:', (cm[0,0]+cm[1,1])/sum(cm.ravel()))\n",
    "    print('sensitivity:', cm[0,0]/(cm[0,0]+cm[1,0]))\n",
    "    print('specificity:', cm[1,1]/(cm[1,1]+cm[0,1]),'\\n')\n",
    "    iter_num=iter_num+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ffa8f5-5f0c-4d5b-b768-086766d451cd",
   "metadata": {},
   "source": [
    "Using a cutoff value of .63 (which approximates the original response distribution) yields a much higher sensitivity in the 70-74% as opposed to the previous values of 60-63%. While accuracy and specificity are slightly reduced, the increase in sensitivity means this model performs better, towards our goal since we see more value in correctly predicting that a host is Super"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469db6cc-03ef-4552-ac88-cf9e468c199d",
   "metadata": {},
   "source": [
    "### Support Vector Machine Model\n",
    "\n",
    "X_sc was scaled using the standard scalar package from sklearn during Logistic Regression and still exists for our use in the SVM Model.  \n",
    "\n",
    "We'll be using a rbf, or Radial Basis Function, kernel.  RBF determines the similarity between two points and results in a value between 0 and 1, with 0 representing two points that are exactly similar.  However, this kernel doesn't scale well to large data sets.  Our initial suspicion is that we'll have an extended run time for our dataset and may find this model training to be impractical for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26946ec-c09d-4454-90b7-89f5c660deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sc,y, random_state=seed[0], train_size=.8, shuffle=True)#X_test/train is scaled since X_sc is used for X\n",
    "    \n",
    "svm_clf = SVC(C=0.5,kernel='rbf',degree =3, gamma='auto', random_state=seed[0])\n",
    "svm_clf.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "y_hat = svm_clf.predict(X_test)\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "cm = mt.confusion_matrix(y_test,y_hat, labels=[1,0])\n",
    "print('Confusion Matrix')\n",
    "print(pd.DataFrame(cm,index= ['True','False'], columns=['True','False']))\n",
    "print('________________________________')\n",
    "print('accuracy:', (cm[0,0]+cm[1,1])/sum(cm.ravel()))\n",
    "print('sensitivity:', cm[0,0]/(cm[0,0]+cm[1,0]))\n",
    "print('specificity:', cm[1,1]/(cm[1,1]+cm[0,1]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2654cc99",
   "metadata": {},
   "source": [
    "In the code above, we begin by creating a train/test split using the scaled data from before (scaled just prior to the logistic classifier), passing in a seed value assigned earlier, and setting a split of 80/20.  We then create an Support Vector Classifier using the sklearn.SVM package with the SVC class.  We train using the fit function on th etraining data.  We then predict on the test data and use those predictions to score against the actual values.\n",
    "\n",
    "This results in thg following model performance:\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "|   | True | False |\n",
    "| ---- | ----- | ---- |\n",
    "| True | 563 | 1459 |\n",
    "| False | 339 | 3755 |\n",
    "\n",
    "________________________________\n",
    "| Metric | Value |\n",
    "| ---- | ---- |\n",
    "| accuracy: | 0.7060170045781556 |\n",
    "| sensitivity:| 0.6241685144124168 |\n",
    "| specificity:| 0.7201764480245493 |\n",
    "\n",
    "Due to the extended time to train, model tuning was impractical and we felt our best next step was to pursue using the SGDClassifier package, along with a GridSearch to optimize and find the best model.  We'll do that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3825eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to tune using Grid Search\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'loss':['hinge'],\n",
    "    'penalty':['l2', 'elasticnet'],\n",
    "    'n_iter_no_change':[5],\n",
    "    'l1_ratio':[0.0,0.05,0.06,0.07,0.08,0.09,0.1,0.12,0.13,0.14,0.15,0.2],\n",
    "    'alpha':[0.1],\n",
    "    'fit_intercept':[True],\n",
    "    'learning_rate':['optimal'],\n",
    "    'n_jobs':[-1],\n",
    "    'random_state':[seed[0]]\n",
    "}\n",
    "\n",
    "sgdGS = GridSearchCV(sgd, parameters, cv=5, verbose=1, n_jobs=1, error_score='raise')\n",
    "\n",
    "sgdGS.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(sgdGS.best_params_)\n",
    "\n",
    "\n",
    "#Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
    "#{'alpha': 0.1, 'fit_intercept': True, 'l1_ratio': 0.0, 'learning_rate': 'optimal', 'loss': 'hinge', 'n_iter_no_change': 5, 'n_jobs': -1, 'penalty': 'l2', 'random_state': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6854b1",
   "metadata": {},
   "source": [
    "Above, we've applied a Grid Search method to determine the best performing model for a SGDClassifier.  We'll use those settings to create a SGD Model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying out SGDClassifier to reduce the time to train.\n",
    "\n",
    "\n",
    "regularize_const = .1\n",
    "iterations = 5\n",
    "#use paramters from gridsearch above\n",
    "svmSGD = SGDClassifier(alpha=regularize_const, fit_intercept=True, learning_rate='optimal',\n",
    "                       loss='hinge', n_iter_no_change=iterations, n_jobs=-1, penalty='l2', random_state=seed[0])\n",
    "\n",
    "svmSGD.fit(X_train, y_train.values.ravel())\n",
    "y_hat = svmSGD.predict(X_test)\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "cm = mt.confusion_matrix(y_test,y_hat, labels=[1,0])\n",
    "print('Confusion Matrix')\n",
    "print(pd.DataFrame(cm,index= ['True','False'], columns=['True','False']))\n",
    "print('________________________________')\n",
    "print('accuracy:', (cm[0,0]+cm[1,1])/sum(cm.ravel()))\n",
    "print('sensitivity:', cm[0,0]/(cm[0,0]+cm[1,0]))\n",
    "print('specificity:', cm[1,1]/(cm[1,1]+cm[0,1]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f20c9",
   "metadata": {},
   "source": [
    "Our SGD Classifier model performance is the following:\n",
    "\n",
    "This results in thg following model performance:\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "|   | True | False |\n",
    "| ---- | ----- | ---- |\n",
    "| True | 372 | 1650 |\n",
    "| False | 237 | 3857 |\n",
    "\n",
    "________________________________\n",
    "| Metric | Value |\n",
    "| ---- | ---- |\n",
    "| accuracy: | 0.6914650098103335 |\n",
    "| sensitivity:| 0.6108374384236454 |\n",
    "| specificity:| 0.7003813328491011  |\n",
    "\n",
    "This was a notable drop, though not excessive, from the SVM Classifier trained above though that ran at roughly 400 times longer than this to train.  In a situation where retraining on new data was frequent, this would be substantial.  Sure, memory and cores could be thrown at this problem now but as the data set grew, such an increase in run time would become unmanagable and impractical quickly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bfe8a4-27e1-45a1-97d0-06231728a192",
   "metadata": {},
   "source": [
    "# Model Advantages\n",
    "\n",
    "### Rubric Note: Discuss the advantages of each model for each classifciation task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficience? Explain in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e863eb4-25ab-43d4-8283-4ecee30b0749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4d8aa-205d-49dc-89e4-13e7ec2b5df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f987ba70-eafc-4c48-820e-2a923b9ae43f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interpret Feature Importance\n",
    "\n",
    "### Rubric Note: Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. WHy do you think some variables are more important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95a153-c13b-4ffe-8ac4-e3b1b328a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficient corresponds to the weights, since the data has been scaled (larger absolute values are features of most importance)\n",
    "importance = pd.DataFrame(lor_clf.coef_.transpose(),index= X.columns.values.tolist(), columns= ['coefficient']).sort_values(ascending=False, by = 'coefficient')\n",
    "importance = importance[(importance['coefficient']<-.16) | (importance['coefficient']>.16)] # since we have so many variables, this takes the most important\n",
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd6f94-2299-4437-8bc6-3234b4c72ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature importance for select features, (16 features with the highest positive and negative coefficients)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "weights = importance.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e3337-4cdb-431c-958f-c7d11d2a70d9",
   "metadata": {},
   "source": [
    "## Due to having several hundred features we narrowed down our search to 10 key features for being a Superhost. For context on a superhost, which is a highly reccomended host for a listing, the requirements are as follows:\n",
    "\n",
    "### Completed at least 10 trips or 3 reservations that total at least 100 nights\n",
    "### Maintained a 90% response rate or higher\n",
    "### Maintained a less than 1% cancellation rate, with exceptions made for those that fall under our Extenuating Circumstances policy\n",
    "### Maintained a 4.8 overall rating (A review counts towards Superhost status when either both the guest and the Host have submitted a review, or the 14-day window for reviews is over, whichever comes first).\n",
    "\n",
    "## The features we found that are the strongest in influencing the probability of becoming a Super Host are review_scores_rating, has_availability, number_of_reviews, property_type_Private room, price, beds, neighbourhood_cleansed_Sherman Oaks, neighbourhood_cleansed_Santa Ana, property_type_Hotel room, room_type_Private room. Review_scores_rating was the highest at 1.721785 and room_type_Private room is the lowest at -0.413266. This indicates that ratings are of utmost imporance in predicting a superhost. Additionally numbers of ratings, availablility, if it's a private room, the price, number of beds and if it's in Sherman Oaks or Santa Ana add influence to that as well. There are also 2 that would negatively affect this as it being a hotel property, or the whole property being a private room. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da7c16-ef28-4758-bebd-57e06dda4612",
   "metadata": {},
   "source": [
    "# Interpret Support Vectors\n",
    "\n",
    "### Rubric Note: Look at the chosen support vectors for the classfication task. Do these provide any 8insight into the data? Explain. IF you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model - then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc71593",
   "metadata": {},
   "source": [
    "To begin, since we went with a stochastic gradient descent classified due to training time, we'll first look at the features that made up the most importance in that model.  After, we'll attempt to train a SVM Classsifier on a subsample of the original data to see if model performance is acceptable and the time to run component becomes more reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d008150-261a-4306-842b-d8ab51a68ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD Feature importance \n",
    "\n",
    "# coefficient corresponds to the weights, since the data has been scaled (larger absolute values are features of most importance)\n",
    "svmImportance = pd.DataFrame(svmSGD.coef_.transpose(),index= X.columns.values.tolist(), columns= ['coefficient']).sort_values(ascending=False, by = 'coefficient')\n",
    "svmImportance = svmImportance[(svmImportance['coefficient']<-.035) | (svmImportance['coefficient']>.035)] # since we have so many variables, this takes the most important\n",
    "svmImportance\n",
    "\n",
    "\n",
    "## feature importance for select features\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "weights = svmImportance.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0814106",
   "metadata": {},
   "source": [
    "#### Sub sampling the data set to attempt to train a SVC to identify the support vectors\n",
    "\n",
    "Because we used an 80/20 train test split, I already have a 20% sub sample in the test.  I will use that as my 'start' sub sample and split that out into a new train test split and use that to train a SVC and identify the support vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aca8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new train/test split using ONLY the test split from before - effectively using a 20% subsample of the data to train an SVC\n",
    "#make copies of the test split into an X_sub, and y_sub for use in this model\n",
    "X_sub = X_test\n",
    "y_sub = y_test\n",
    "\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X_sub,y_sub, random_state=seed[0], train_size=.8, shuffle=True)#X_test/train is scaled since X_sc is used for X\n",
    "    \n",
    "svm_clf_sub = SVC(C=0.5,kernel='rbf',degree =3, gamma='auto', random_state=seed[0])\n",
    "svm_clf_sub.fit(X_train_sub,y_train_sub.values.ravel())\n",
    "\n",
    "y_hat_sub = svm_clf_sub.predict(X_test_sub)\n",
    "acc = mt.accuracy_score(y_test_sub,y_hat_sub)\n",
    "cm = mt.confusion_matrix(y_test_sub,y_hat_sub, labels=[1,0])\n",
    "print('Confusion Matrix')\n",
    "print(pd.DataFrame(cm,index= ['True','False'], columns=['True','False']))\n",
    "print('________________________________')\n",
    "print('accuracy:', (cm[0,0]+cm[1,1])/sum(cm.ravel()))\n",
    "print('sensitivity:', cm[0,0]/(cm[0,0]+cm[1,0]))\n",
    "print('specificity:', cm[1,1]/(cm[1,1]+cm[0,1]),'\\n')\n",
    "\n",
    "print(X_sub.shape)\n",
    "print(svm_clf_sub.support_vectors_.shape)\n",
    "print(svm_clf_sub.support_.shape)\n",
    "print(svm_clf_sub.n_support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14f3b7",
   "metadata": {},
   "source": [
    "As mentioned above, we want to be able to analyze the support vectors used by a SVM on our data.  To do so, we made the decision to use a 20% subsample of our data.  Because we had already split out the data into an 80/20 training/testing split, we were able to use the test split, already 20%, as a feed into our SVM Classifier model.\n",
    "\n",
    "We begin by creating a train/test split from that 20% subsample.  We then fit and predict on that mode.\n",
    "\n",
    "This results in thg following model performance:\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "|   | True | False |\n",
    "| ---- | ----- | ---- |\n",
    "| True | 60 | 325 |\n",
    "| False | 43 | 796 |\n",
    "\n",
    "________________________________\n",
    "| Metric | Value |\n",
    "| ---- | ---- |\n",
    "| accuracy: | 0.6993464052287581 |\n",
    "| sensitivity:| 0.5825242718446602 |\n",
    "| specificity:|  0.7100802854594113  |\n",
    "\n",
    "The results show that the performance is in line with the SGD and not too much below the SVC trained and tested with the full dataset.  However, the training time was 50 times less.  This alone gives support to the viability of this model.\n",
    "\n",
    "Next, we'll look at the instances choosen as support vectors by the support vector classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9329b54",
   "metadata": {},
   "source": [
    "#### Look at the instances choosen as support vectors\n",
    "\n",
    "Now that I have an SVM trained on a subsample of the data that is running in about 7 seconds, I will evaluate the instances that were chosen as support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract out the instances as support vectors and plot them\n",
    "df_tested_on = df.iloc[X_train_sub.index.values,:].copy()\n",
    "df_support = df_tested_on.iloc[svm_clf_sub.support_,:].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8f7cc-e693-4ca0-8e6d-c2884c8b0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import boxplot\n",
    "df_grouped_support = df_support.groupby(['host_is_superhost'])\n",
    "df_grouped = df.groupby(['host_is_superhost'])\n",
    "\n",
    "vars_to_plot = ['number_of_reviews','review_scores_rating','has_availability','calculated_host_listings_count','price','bedrooms']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend([0,1])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend([0,1])\n",
    "    plt.title(v+' (Original)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39701a-91ec-484f-abc6-1c03d1423e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "83c973882773d9941208e9edf3746a11bb3e141378b738d1f5e8877c84800a42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
