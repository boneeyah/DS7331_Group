{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c417ca4e-c052-44e2-9f4b-3e0c84e88cdc",
   "metadata": {},
   "source": [
    "#### data wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9722752c-e0e1-4862-bece-058690ae9e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "df = pd.read_csv(\"https://media.githubusercontent.com/media/boneeyah/DS7331_Group/main/Data_Files/airbnb_los_angeles.csv\")\n",
    "#df = pd.read_csv(\"Data_Files/airbnb_los_angeles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e8676f5-9c08-472a-94b0-7cf8ac42ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop variables which won't be useful for the classification model\n",
    "for col in [\n",
    "    'listing_url','scrape_id','last_scraped','description','neighborhood_overview','picture_url','host_url','host_about','host_response_time','host_response_rate','host_acceptance_rate',\n",
    "    'host_thumbnail_url','host_picture_url','host_verifications','host_has_profile_pic','bathroom_text','host_listings_count','host_neighbourhood','bathrooms','minimum_minimum_nights',\n",
    "    'maximum_minimum_nights','minimum_maximum_nights','maximum_maximum_nights','minimum_nights_avg_ntm','maximum_nights_avg_ntm','calendar_updated','availability_30','availability_60',\n",
    "    'availability_90','availability_365','calendar_last_scraped','number_of_reviews_ltm','number_of_reviews_l30d','review_scores_accuracy','review_scores_communication','review_scores_cleanliness',\n",
    "    'review_scores_checkin','review_scores_value','review_scores_location','calculated_host_listings_count_entire_homes','calculated_host_listings_count_private_rooms',\n",
    "    'calculated_host_listings_count_shared_rooms','reviews_per_month','neighbourhood','neighbourhood_group_cleansed', 'first_review','last_review','minimum_nights','maximum_nights','license','host_total_listings_count'\n",
    "]:\n",
    "    if col in df:\n",
    "        del df[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84031bd0-0813-4264-81fa-f3548cb72b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entire unit     22767\n",
       "Private room     8081\n",
       "Shared room       510\n",
       "Room              401\n",
       "other             298\n",
       "Hotel room        272\n",
       "Name: property_type, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove nulls\n",
    "df = df[~df.review_scores_rating.isnull() & ~df.bathrooms_text.isnull() & ~df.host_since.isnull() & ~df.host_location.isnull()]\n",
    "\n",
    "## getting property type from string\n",
    "types = ['Private room', 'Entire', 'Room in hotel','Room','Shared room']\n",
    "pat = '|'.join(r\"\\b{}\\b\".format(x) for x in types)\n",
    "\n",
    "df['property_type']= df['property_type'].str.extract('('+ pat + ')', expand = False)\n",
    "df['property_type'] = (df.property_type.\n",
    "                       fillna(value = 'other').\n",
    "                       replace(['Entire','Room in hotel'],['Entire unit','Hotel room']))\n",
    "df['property_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e58420e-ee3a-4444-a029-c8234aad0126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute values based on median\n",
    "df['beds'] = df[['accommodates','beds']].groupby(by = 'accommodates').transform(lambda grp: grp.fillna(grp.median()))\n",
    "df_grouped = df.groupby(by = ['property_type','beds'])\n",
    "df_imputed = df_grouped[['beds','bedrooms']].transform(lambda grp: grp.fillna(grp.median()))\n",
    "\n",
    "index = df_imputed[df_imputed.bedrooms.isnull()].index\n",
    "df = df.drop(index= index)\n",
    "\n",
    "df['imputed']=df_imputed[['bedrooms']]\n",
    "\n",
    "# replace 'bedrooms' column with imputed column and deleting the duplicated column\n",
    "df['bedrooms'] = df['imputed']\n",
    "del df['imputed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e63e7586-90a1-4694-aa9f-54a8bdf0c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now fixing dtypes for attributes\n",
    "df['host_since'] = pd.to_datetime(df.host_since)\n",
    "df['price'] = df['price'].replace('[\\$,]','',regex = True).astype(float)\n",
    "df['bathrooms_text'] = df['bathrooms_text'].replace(['Half-bath', 'Shared half-bath', 'Private half-bath'],['0.5 bath','0.5 shared bath', '0.5 private bath'])\n",
    "df_bathrooms = df['bathrooms_text'].str.split(n=1, expand=True).rename(columns = {0:'bathroom_number',1:'bathroom_type'})\n",
    "df_bathrooms['bathroom_type'] = df_bathrooms.bathroom_type.fillna(value = 'bath')\n",
    "df_bathrooms['bathroom_type'] = df_bathrooms['bathroom_type'].replace(['baths','shared baths'],['bath','shared bath'])\n",
    "df_bathrooms['bathroom_number'] = df_bathrooms['bathroom_number'].astype('float')\n",
    "df.insert(15, 'bathroom_number',df_bathrooms['bathroom_number'])\n",
    "df.insert(16, 'bathroom_type', df_bathrooms['bathroom_type'])\n",
    "del df['bathrooms_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6705188f-0607-439b-9898-e239313cb693",
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter out price outliers\n",
    "df = df[(df.beds<10) & (df.price<750)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ef94df2-03a6-4c1f-a16b-8f37ecc0d208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30580 entries, 0 to 30579\n",
      "Data columns (total 25 columns):\n",
      " #   Column                          Non-Null Count  Dtype         \n",
      "---  ------                          --------------  -----         \n",
      " 0   id                              30580 non-null  int64         \n",
      " 1   name                            30580 non-null  object        \n",
      " 2   host_id                         30580 non-null  int64         \n",
      " 3   host_name                       30580 non-null  object        \n",
      " 4   host_since                      30580 non-null  datetime64[ns]\n",
      " 5   host_location                   30580 non-null  object        \n",
      " 6   host_is_superhost               30580 non-null  object        \n",
      " 7   host_identity_verified          30580 non-null  object        \n",
      " 8   neighbourhood_cleansed          30580 non-null  object        \n",
      " 9   latitude                        30580 non-null  float64       \n",
      " 10  longitude                       30580 non-null  float64       \n",
      " 11  property_type                   30580 non-null  object        \n",
      " 12  room_type                       30580 non-null  object        \n",
      " 13  accommodates                    30580 non-null  int64         \n",
      " 14  bathroom_number                 30580 non-null  float64       \n",
      " 15  bathroom_type                   30580 non-null  object        \n",
      " 16  bedrooms                        30580 non-null  float64       \n",
      " 17  beds                            30580 non-null  float64       \n",
      " 18  amenities                       30580 non-null  object        \n",
      " 19  price                           30580 non-null  float64       \n",
      " 20  has_availability                30580 non-null  object        \n",
      " 21  number_of_reviews               30580 non-null  int64         \n",
      " 22  review_scores_rating            30580 non-null  float64       \n",
      " 23  instant_bookable                30580 non-null  object        \n",
      " 24  calculated_host_listings_count  30580 non-null  int64         \n",
      "dtypes: datetime64[ns](1), float64(7), int64(5), object(12)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "436fc516-506b-407a-92c2-42b1be9fb92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>30580.0</td>\n",
       "      <td>33.993072</td>\n",
       "      <td>0.189359</td>\n",
       "      <td>33.33865</td>\n",
       "      <td>33.903087</td>\n",
       "      <td>34.041553</td>\n",
       "      <td>34.099513</td>\n",
       "      <td>34.81326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accommodates</th>\n",
       "      <td>30580.0</td>\n",
       "      <td>3.750621</td>\n",
       "      <td>2.484750</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>16.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bedrooms</th>\n",
       "      <td>30580.0</td>\n",
       "      <td>1.616334</td>\n",
       "      <td>0.932373</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price</th>\n",
       "      <td>30580.0</td>\n",
       "      <td>186.166906</td>\n",
       "      <td>141.618829</td>\n",
       "      <td>10.00000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>749.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>number_of_reviews</th>\n",
       "      <td>30580.0</td>\n",
       "      <td>47.849052</td>\n",
       "      <td>81.192747</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>1512.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calculated_host_listings_count</th>\n",
       "      <td>30580.0</td>\n",
       "      <td>13.259320</td>\n",
       "      <td>45.123611</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>532.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  count        mean         std       min  \\\n",
       "latitude                        30580.0   33.993072    0.189359  33.33865   \n",
       "accommodates                    30580.0    3.750621    2.484750   1.00000   \n",
       "bedrooms                        30580.0    1.616334    0.932373   1.00000   \n",
       "price                           30580.0  186.166906  141.618829  10.00000   \n",
       "number_of_reviews               30580.0   47.849052   81.192747   1.00000   \n",
       "calculated_host_listings_count  30580.0   13.259320   45.123611   1.00000   \n",
       "\n",
       "                                      25%         50%         75%         max  \n",
       "latitude                        33.903087   34.041553   34.099513    34.81326  \n",
       "accommodates                     2.000000    3.000000    5.000000    16.00000  \n",
       "bedrooms                         1.000000    1.000000    2.000000     8.00000  \n",
       "price                           85.000000  140.000000  245.000000   749.00000  \n",
       "number_of_reviews                4.000000   15.000000   55.000000  1512.00000  \n",
       "calculated_host_listings_count   1.000000    2.000000    7.000000   532.00000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# breakdown of categorical variables with number of levels (unique)\n",
    "df.iloc[:,[1,3,5,6,7,9,12,13,16,19,21,24]].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de56075-d76d-4607-a50a-9cedc07fe04b",
   "metadata": {},
   "source": [
    "# Create Models\n",
    "\n",
    "\n",
    "### Rubric Note: Create a logistic regression model and a support vector machine model for the classification task involved with your dataset. Assess how well each model performs (use a 80/20 train/test split for your data). Adjust parameters of the model to make them more accurate. If your dataset size requires the use of stochastic gradient descent, then linear kernel only is fine to use. THat is, the SGDClassifier is fine to use for optimizing logistic regression and linear support vector machines. FOr many problems, SGD will be required in order to train the SVM model in a reasonable timeframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843368e8-f02d-40f0-abca-06a0c57cd36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model will focus on classifying superhost status\n",
    "# since we're encoding with binary response, we can use labelencoder from sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['host_is_superhost'] = label_encoder.fit_transform(df['host_is_superhost'])\n",
    "df.host_is_superhost.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e211631-a474-45c2-bb8c-34bb9a4ea7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## encoding binary categorical variables\n",
    "# encode identity verified\n",
    "df['host_identity_verified'] = label_encoder.fit_transform(df['host_identity_verified'])\n",
    "df.host_identity_verified.value_counts()\n",
    "\n",
    "#encode has_availability\n",
    "df['has_availability'] = label_encoder.fit_transform(df['has_availability'])\n",
    "df.has_availability.value_counts()\n",
    "\n",
    "#encode instant bookable\n",
    "df['instant_bookable'] = label_encoder.fit_transform(df['instant_bookable'])\n",
    "df.instant_bookable.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ef38fe-7d74-4925-b8c8-63597a58be4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing host_location to a binary feature of is_local using a list of LA area neighborhoods that will be used to extract from location str\n",
    "#los_angeles = pd.read_csv('Data_Files/LosAngelesNeighborhoods.csv')\n",
    "los_angeles = pd.read_csv('https://raw.githubusercontent.com/boneeyah/DS7331_Group/main/Data_Files/LosAngelesNeighborhoods.csv')\n",
    "los_angeles = los_angeles.iloc[:,0].tolist()\n",
    "\n",
    "pattern = '|'.join(los_angeles)\n",
    "df['host_is_local'] = df['host_location'].str.contains(pattern)\n",
    "\n",
    "df['host_is_local'] = label_encoder.fit_transform(df['host_is_local'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be9bb4-5b2e-4ef2-a251-c4d88f1dd246",
   "metadata": {},
   "outputs": [],
   "source": [
    "## changing host_since to host_for to get a a numerical variable that corresponds to length in months\n",
    "## data is from June 6 2022, 06-06-2022\n",
    "end_date = pd.to_datetime('06-06-2022', format= \"%m-%d-%Y\")\n",
    "df['host_for'] = (end_date-df.host_since)/np.timedelta64(1,'M')\n",
    "\n",
    "df = df.drop(columns= ['host_location', 'host_since']) #drop old host_location and host_since columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1c92c-c941-4257-aab5-cf4528792798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding other categorical variables as a non-sparse dataframe\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "df_temp = df[['id','neighbourhood_cleansed','property_type', 'room_type', 'bathroom_type']] # to get only the cat variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c799bcac-826c-4c0b-be36-f697cf28c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(drop='first',sparse=False)\n",
    "feature_arr = ohe.fit_transform(df_temp[['neighbourhood_cleansed','property_type', 'room_type', 'bathroom_type']])\n",
    "feature_labels = ohe.get_feature_names_out()\n",
    "\n",
    "#create dataframe with features\n",
    "df_temp = pd.DataFrame(feature_arr, columns= feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec75e9-070e-450a-9a28-11a916f1ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop original columns and join with new onehotencoded columns\n",
    "df = df.drop(columns = ['neighbourhood_cleansed','property_type', 'room_type', 'bathroom_type','amenities','name','host_name','id','host_id','latitude','longitude']).join(df_temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefb4eeb-fea3-43f3-b4f2-238a5baa6c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### different method I found first, commented out for now, this could be used to transform within the original dataset saving one step\n",
    "### but it's easy enough to just fit a new dataframe and then add to original and drop original columns instead\n",
    "#transformer = make_column_transformer((OneHotEncoder(drop = 'first',sparse=False), ['neighbourhood_cleansed','property_type', 'room_type', 'bathroom_type']), remainder='passthrough')\n",
    "#df_temp = transformer.fit_transform(df_temp)\n",
    "#df_temp = pd.DataFrame(df_temp, columns=transformer.get_feature_names_out())\n",
    "#print(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7387da7-ff79-4f7c-98d9-56b7b97a8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean up temporary files\n",
    "del df_temp, feature_arr, feature_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83169115-f751-4728-b3ae-15cda28cbd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94150f29-3865-4d72-a8dd-5b4d1fc290d8",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374dac3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages we'll be using in the model building sections.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#dividing dataset into features and labels and using a standard scaler to scale the x values\n",
    "X = df.loc[:,df.columns != 'host_is_superhost']\n",
    "X_sc = StandardScaler().fit_transform(X) #scaled X values\n",
    "X_sc = pd.DataFrame(X_sc,columns=X.columns)\n",
    "y = df.loc[:, df.columns == 'host_is_superhost']\n",
    "\n",
    "\n",
    "seed = [10,2,2022] #seed to be used, it will be assigned as random_state when splitting instead of using np to set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0017f-b9b2-43fd-bba7-b5420a7644ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### performing 80/20 split using sklearn\n",
    "\n",
    "\n",
    "cutoff = .63\n",
    "\n",
    "\n",
    "iter_num =0\n",
    "for i in seed:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_sc,y, random_state=i, train_size=.8, shuffle=True) #X_train is scaled since X_sc is used for X\n",
    "    lor_clf = LogisticRegressionCV(cv=10, max_iter = 300, random_state = i)\n",
    "    lor_clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    y_hat = lor_clf.predict_proba(X_test)\n",
    "\n",
    "\n",
    "    response = pd.DataFrame(y_hat, columns=['F','T'])\n",
    "    y_preds=(response['T']>cutoff).astype('int')\n",
    "\n",
    "    cm = confusion_matrix(y_test,y_preds, labels=[1,0])\n",
    "    print('---------Iteration  ',iter_num,'---------')\n",
    "    print('')\n",
    "    print('Confusion Matrix')\n",
    "    print(pd.DataFrame(cm,index= ['True','False'], columns=['True','False']))\n",
    "    print('________________________________')\n",
    "    print('accuracy:', (cm[0,0]+cm[1,1])/sum(cm.ravel()))\n",
    "    print('sensitivity:', cm[0,0]/(cm[0,0]+cm[1,0]))\n",
    "    print('specificity:', cm[1,1]/(cm[1,1]+cm[0,1]),'\\n')\n",
    "    iter_num=iter_num+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469db6cc-03ef-4552-ac88-cf9e468c199d",
   "metadata": {},
   "source": [
    "### Support Vector Machine Model\n",
    "\n",
    "X_sc was scaled using the standard scalar package from sklearn during Logistic Regression and still exists for our use in the SVM Model.  \n",
    "\n",
    "We'll be using a rbf, or Radial Basis Function, kernel.  RBF determines the similarity between two points and results in a value between 0 and 1, with 0 representing two points that are exactly similar.  However, this kernel doesn't scale well to large data sets.  Our initial suspicion is that we'll have an extended run time for our dataset and may find this model training to be impractical for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26946ec-c09d-4454-90b7-89f5c660deac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics as mt\n",
    "from matplotlib import pyplot as plt\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sc,y, random_state=seed[0], train_size=.8, shuffle=True)#X_test/train is scaled since X_sc is used for X\n",
    "    \n",
    "svm_clf = SVC(C=0.5,kernel='rbf',degree =3, gamma='auto', random_state=seed[0])\n",
    "svm_clf.fit(X_train,y_train.values.ravel())\n",
    "\n",
    "y_hat = svm_clf.predict(X_test)\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "cm = mt.confusion_matrix(y_test,y_hat, labels=[1,0])\n",
    "print('Confusion Matrix')\n",
    "print(pd.DataFrame(cm,index= ['True','False'], columns=['True','False']))\n",
    "print('________________________________')\n",
    "print('accuracy:', (cm[0,0]+cm[1,1])/sum(cm.ravel()))\n",
    "print('sensitivity:', cm[0,0]/(cm[0,0]+cm[1,0]))\n",
    "print('specificity:', cm[1,1]/(cm[1,1]+cm[0,1]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2654cc99",
   "metadata": {},
   "source": [
    "In the code above, we begin by creating a train/test split using the scaled data from before (scaled just prior to the logistic classifier), passing in a seed value assigned earlier, and setting a split of 80/20.  We then create an Support Vector Classifier using the sklearn.SVM package with the SVC class.  We train using the fit function on th etraining data.  We then predict on the test data and use those predictions to score against the actual values.\n",
    "\n",
    "This results in thg following model performance:\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "|   | True | False |\n",
    "| ---- | ----- | ---- |\n",
    "| True | 563 | 1459 |\n",
    "| False | 339 | 3755 |\n",
    "\n",
    "________________________________\n",
    "| Metric | Value |\n",
    "| ---- | ---- |\n",
    "| accuracy: | 0.7060170045781556 |\n",
    "| sensitivity:| 0.6241685144124168 |\n",
    "| specificity:| 0.7201764480245493 |\n",
    "\n",
    "Due to the extended time to train, model tuning was impractical and we felt our best next step was to pursue using the SGDClassifier package, along with a GridSearch to optimize and find the best model.  We'll do that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3825eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to tune using Grid Search\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "sgd = SGDClassifier()\n",
    "\n",
    "parameters = {\n",
    "    'loss':['hinge'],\n",
    "    'penalty':['l2', 'elasticnet'],\n",
    "    'n_iter_no_change':[5],\n",
    "    'l1_ratio':[0.0,0.05,0.06,0.07,0.08,0.09,0.1,0.12,0.13,0.14,0.15,0.2],\n",
    "    'alpha':[0.1],\n",
    "    'fit_intercept':[True],\n",
    "    'learning_rate':['optimal'],\n",
    "    'n_jobs':[-1],\n",
    "    'random_state':[seed[0]]\n",
    "}\n",
    "\n",
    "sgdGS = GridSearchCV(sgd, parameters, cv=5, verbose=1, n_jobs=1, error_score='raise')\n",
    "\n",
    "sgdGS.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "print(sgdGS.best_params_)\n",
    "\n",
    "\n",
    "#Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
    "#{'alpha': 0.1, 'fit_intercept': True, 'l1_ratio': 0.0, 'learning_rate': 'optimal', 'loss': 'hinge', 'n_iter_no_change': 5, 'n_jobs': -1, 'penalty': 'l2', 'random_state': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6854b1",
   "metadata": {},
   "source": [
    "Above, we've applied a Grid Search method to determine the best performing model for a SGDClassifier.  We'll use those settings to create a SGD Model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed2358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying out SGDClassifier to reduce the time to train.\n",
    "\n",
    "\n",
    "regularize_const = .1\n",
    "iterations = 5\n",
    "#use paramters from gridsearch above\n",
    "svmSGD = SGDClassifier(alpha=regularize_const, fit_intercept=True, learning_rate='optimal',\n",
    "                       loss='hinge', n_iter_no_change=iterations, n_jobs=-1, penalty='l2', random_state=seed[0])\n",
    "\n",
    "svmSGD.fit(X_train, y_train.values.ravel())\n",
    "y_hat = svmSGD.predict(X_test)\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "cm = mt.confusion_matrix(y_test,y_hat, labels=[1,0])\n",
    "print('Confusion Matrix')\n",
    "print(pd.DataFrame(cm,index= ['True','False'], columns=['True','False']))\n",
    "print('________________________________')\n",
    "print('accuracy:', (cm[0,0]+cm[1,1])/sum(cm.ravel()))\n",
    "print('sensitivity:', cm[0,0]/(cm[0,0]+cm[1,0]))\n",
    "print('specificity:', cm[1,1]/(cm[1,1]+cm[0,1]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8f20c9",
   "metadata": {},
   "source": [
    "Our SGD Classifier model performance is the following:\n",
    "\n",
    "This results in thg following model performance:\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "|   | True | False |\n",
    "| ---- | ----- | ---- |\n",
    "| True | 372 | 1650 |\n",
    "| False | 237 | 3857 |\n",
    "\n",
    "________________________________\n",
    "| Metric | Value |\n",
    "| ---- | ---- |\n",
    "| accuracy: | 0.6914650098103335 |\n",
    "| sensitivity:| 0.6108374384236454 |\n",
    "| specificity:| 0.7003813328491011  |\n",
    "\n",
    "This was a notable drop, though not excessive, from the SVM Classifier trained above though that ran at roughly 400 times longer than this to train.  In a situation where retraining on new data was frequent, this would be substantial.  Sure, memory and cores could be thrown at this problem now but as the data set grew, such an increase in run time would become unmanagable and impractical quickly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bfe8a4-27e1-45a1-97d0-06231728a192",
   "metadata": {},
   "source": [
    "# Model Advantages\n",
    "\n",
    "### Rubric Note: Discuss the advantages of each model for each classifciation task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficience? Explain in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e863eb4-25ab-43d4-8283-4ecee30b0749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4d8aa-205d-49dc-89e4-13e7ec2b5df8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f987ba70-eafc-4c48-820e-2a923b9ae43f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interpret Feature Importance\n",
    "\n",
    "### Rubric Note: Use the weights from logistic regression to interpret the importance of different features for the classification task. Explain your interpretation in detail. WHy do you think some variables are more important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d95a153-c13b-4ffe-8ac4-e3b1b328a4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coefficient corresponds to the weights, since the data has been scaled (larger absolute values are features of most importance)\n",
    "importance = pd.DataFrame(lor_clf.coef_.transpose(),index= X.columns.values.tolist(), columns= ['coefficient']).sort_values(ascending=False, by = 'coefficient')\n",
    "importance = importance[(importance['coefficient']<-.16) | (importance['coefficient']>.16)] # since we have so many variables, this takes the most important\n",
    "importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcd6f94-2299-4437-8bc6-3234b4c72ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## feature importance for select features, (16 features with the highest positive and negative coefficients)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "weights = importance.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84e3337-4cdb-431c-958f-c7d11d2a70d9",
   "metadata": {},
   "source": [
    "## Due to having several hundred features we narrowed down our search to 10 key features for being a Superhost. For context on a superhost, which is a highly reccomended host for a listing, the requirements are as follows:\n",
    "\n",
    "### Completed at least 10 trips or 3 reservations that total at least 100 nights\n",
    "### Maintained a 90% response rate or higher\n",
    "### Maintained a less than 1% cancellation rate, with exceptions made for those that fall under our Extenuating Circumstances policy\n",
    "### Maintained a 4.8 overall rating (A review counts towards Superhost status when either both the guest and the Host have submitted a review, or the 14-day window for reviews is over, whichever comes first).\n",
    "\n",
    "## The features we found that are the strongest in influencing the probability of becoming a Super Host are review_scores_rating, has_availability, number_of_reviews, property_type_Private room, price, beds, neighbourhood_cleansed_Sherman Oaks, neighbourhood_cleansed_Santa Ana, property_type_Hotel room, room_type_Private room. Review_scores_rating was the highest at 1.721785 and room_type_Private room is the lowest at -0.413266. This indicates that ratings are of utmost imporance in predicting a superhost. Additionally numbers of ratings, availablility, if it's a private room, the price, number of beds and if it's in Sherman Oaks or Santa Ana add influence to that as well. There are also 2 that would negatively affect this as it being a hotel property, or the whole property being a private room. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da7c16-ef28-4758-bebd-57e06dda4612",
   "metadata": {},
   "source": [
    "# Interpret Support Vectors\n",
    "\n",
    "### Rubric Note: Look at the chosen support vectors for the classfication task. Do these provide any 8insight into the data? Explain. IF you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model - then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc71593",
   "metadata": {},
   "source": [
    "To begin, since we went with a stochastic gradient descent classified due to training time, we'll first look at the features that made up the most importance in that model.  After, we'll attempt to train a SVM Classsifier on a subsample of the original data to see if model performance is acceptable and the time to run component becomes more reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d008150-261a-4306-842b-d8ab51a68ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD Feature importance \n",
    "\n",
    "# coefficient corresponds to the weights, since the data has been scaled (larger absolute values are features of most importance)\n",
    "svmImportance = pd.DataFrame(svmSGD.coef_.transpose(),index= X.columns.values.tolist(), columns= ['coefficient']).sort_values(ascending=False, by = 'coefficient')\n",
    "svmImportance = svmImportance[(svmImportance['coefficient']<-.035) | (svmImportance['coefficient']>.035)] # since we have so many variables, this takes the most important\n",
    "svmImportance\n",
    "\n",
    "\n",
    "## feature importance for select features\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "weights = svmImportance.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0814106",
   "metadata": {},
   "source": [
    "#### Sub sampling the data set to attempt to train a SVC to identify the support vectors\n",
    "\n",
    "Because we used an 80/20 train test split, I already have a 20% sub sample in the test.  I will use that as my 'start' sub sample and split that out into a new train test split and use that to train a SVC and identify the support vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aca8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new train/test split using ONLY the test split from before - effectively using a 20% subsample of the data to train an SVC\n",
    "#make copies of the test split into an X_sub, and y_sub for use in this model\n",
    "X_sub = X_test\n",
    "y_sub = y_test\n",
    "\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X_sub,y_sub, random_state=seed[0], train_size=.8, shuffle=True)#X_test/train is scaled since X_sc is used for X\n",
    "    \n",
    "svm_clf_sub = SVC(C=0.5,kernel='rbf',degree =3, gamma='auto', random_state=seed[0])\n",
    "svm_clf_sub.fit(X_train_sub,y_train_sub.values.ravel())\n",
    "\n",
    "y_hat_sub = svm_clf_sub.predict(X_test_sub)\n",
    "acc = mt.accuracy_score(y_test_sub,y_hat_sub)\n",
    "cm = mt.confusion_matrix(y_test_sub,y_hat_sub, labels=[1,0])\n",
    "print('Confusion Matrix')\n",
    "print(pd.DataFrame(cm,index= ['True','False'], columns=['True','False']))\n",
    "print('________________________________')\n",
    "print('accuracy:', (cm[0,0]+cm[1,1])/sum(cm.ravel()))\n",
    "print('sensitivity:', cm[0,0]/(cm[0,0]+cm[1,0]))\n",
    "print('specificity:', cm[1,1]/(cm[1,1]+cm[0,1]),'\\n')\n",
    "\n",
    "print(X_sub.shape)\n",
    "print(svm_clf_sub.support_vectors_.shape)\n",
    "print(svm_clf_sub.support_.shape)\n",
    "print(svm_clf_sub.n_support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14f3b7",
   "metadata": {},
   "source": [
    "As mentioned above, we want to be able to analyze the support vectors used by a SVM on our data.  To do so, we made the decision to use a 20% subsample of our data.  Because we had already split out the data into an 80/20 training/testing split, we were able to use the test split, already 20%, as a feed into our SVM Classifier model.\n",
    "\n",
    "We begin by creating a train/test split from that 20% subsample.  We then fit and predict on that mode.\n",
    "\n",
    "This results in thg following model performance:\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "|   | True | False |\n",
    "| ---- | ----- | ---- |\n",
    "| True | 60 | 325 |\n",
    "| False | 43 | 796 |\n",
    "\n",
    "________________________________\n",
    "| Metric | Value |\n",
    "| ---- | ---- |\n",
    "| accuracy: | 0.6993464052287581 |\n",
    "| sensitivity:| 0.5825242718446602 |\n",
    "| specificity:|  0.7100802854594113  |\n",
    "\n",
    "The results show that the performance is in line with the SGD and not too much below the SVC trained and tested with the full dataset.  However, the training time was 50 times less.  This alone gives support to the viability of this model.\n",
    "\n",
    "Next, we'll look at the instances choosen as support vectors by the support vector classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9329b54",
   "metadata": {},
   "source": [
    "#### Look at the instances choosen as support vectors\n",
    "\n",
    "Now that I have an SVM trained on a subsample of the data that is running in about 7 seconds, I will evaluate the instances that were chosen as support vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49b55d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract out the instances as support vectors and plot them\n",
    "df_tested_on = df.iloc[X_train_sub.index.values,:].copy()\n",
    "df_support = df_tested_on.iloc[svm_clf_sub.support_,:].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e8f7cc-e693-4ca0-8e6d-c2884c8b0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import boxplot\n",
    "df_grouped_support = df_support.groupby(['host_is_superhost'])\n",
    "df_grouped = df.groupby(['host_is_superhost'])\n",
    "\n",
    "vars_to_plot = ['number_of_reviews','review_scores_rating','has_availability','calculated_host_listings_count','price','bedrooms']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend([0,1])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend([0,1])\n",
    "    plt.title(v+' (Original)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39701a-91ec-484f-abc6-1c03d1423e3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "83c973882773d9941208e9edf3746a11bb3e141378b738d1f5e8877c84800a42"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
